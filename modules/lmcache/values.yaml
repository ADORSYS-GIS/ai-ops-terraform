inferenceService:
  model:
    image: "lmcache/vllm-openai:${image_tag}"
    storageUri: "${storage_uri}"
