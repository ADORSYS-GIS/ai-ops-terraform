inferenceService:
  replicaCount: ${replica_count}
  model:
    image: "lmcache/vllm-openai:${image_tag}"
    storageUri: "${storage_uri}"
  resources:
    requests:
      memory: "${resources.requests.memory}"
      cpu: "${resources.requests.cpu}"
      nvidia.com/gpu: "${resources.requests.gpu}"
    limits:
      memory: "${resources.limits.memory}"
      cpu: "${resources.limits.cpu}"
      nvidia.com/gpu: "${resources.limits.gpu}"

lmcache:
  chunkSize: "${lmcache.chunkSize}"
  localCpu: "${lmcache.localCpu}"
  localDisk: "${lmcache.localDisk}"
  maxLocalDiskSize: "${lmcache.maxLocalDiskSize}"
  remoteUrl: "${lmcache.remoteUrl}"
  enableP2P: "${lmcache.enableP2P}"
%{ if lmcache.lookupUrl != null ~}
  lookupUrl: "${lmcache.lookupUrl}"
%{ endif ~}
%{ if lmcache.distributedUrl != null ~}
  distributedUrl: "${lmcache.distributedUrl}"
%{ endif ~}

ingress:
  enabled: ${ingress.enabled}
  hosts: ${yamlencode(ingress.hosts)}
  tls: ${yamlencode(ingress.tls)}
