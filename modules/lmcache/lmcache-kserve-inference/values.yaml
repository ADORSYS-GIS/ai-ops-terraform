inferenceService:
  name: "lmcache-inference-service"
  namespace: "${namespace}"
  model:
    # Use the official lmcache/vllm-openai image, or your custom image
    image: "lmcache/vllm-openai:${image_tag}"
    imagePullPolicy: IfNotPresent
    storageUri: "${storage_uri}"
  
lmcache:
  chunkSize: "256"
  localCpu: "True"

httpRoute:
  enabled: false

ingress:
  enabled: false

service:
  type: ClusterIP
  port: 80
